{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experts RS Main\n",
    "This note book is for running our ExpertsRS prototype system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the API\n",
    "Set up your LLM api in EXPERTS_RS_CONFIG_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from prompts import scientist_prompt, engineer_prompt, manager_prompt, user_proxy_prompt, executor_prompt\n",
    "\n",
    "config_list = autogen.config_list_from_json(\"EXPERTS_RS_CONFIG_LIST\") # set the api key in the config list\n",
    "print(\"LLM models: \", [config_list[i][\"model\"] for i in range(len(config_list))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm config\n",
    "gpt4_config = {\n",
    "    \"cache_seed\": 9,  # change the cache_seed for different trials\n",
    "    \"temperature\": 0,\n",
    "    \"config_list\": config_list,\n",
    "    \"timeout\": 120, # in seconds\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Agents\n",
    "Construct agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The User proxy\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    system_message= user_proxy_prompt,\n",
    "    code_execution_config=False,\n",
    "    human_input_mode=\"ALWAYS\",\n",
    ")\n",
    "\n",
    "# The Manager\n",
    "manager = autogen.AssistantAgent(\n",
    "    name=\"Manager\",\n",
    "    system_message= manager_prompt,\n",
    "    llm_config=gpt4_config\n",
    ")\n",
    "\n",
    "# The Scientist\n",
    "scientist = autogen.AssistantAgent(\n",
    "    name=\"Scientist\",\n",
    "    system_message= scientist_prompt,\n",
    "    llm_config=gpt4_config\n",
    ")\n",
    "\n",
    "# The Engineer\n",
    "engineer = autogen.AssistantAgent(\n",
    "    name=\"Engineer\",\n",
    "    system_message=engineer_prompt,\n",
    "    llm_config=gpt4_config,\n",
    "    code_execution_config=False,  # Turn off code execution for this agent.\n",
    "\n",
    ")\n",
    "\n",
    "# The Executor\n",
    "executor = autogen.UserProxyAgent(\n",
    "    name=\"Executor\",\n",
    "    system_message=executor_prompt,\n",
    "    human_input_mode=\"NEVER\",  # auto response mode\n",
    "     code_execution_config={\n",
    "        \"executor\": \"commandline-local\",  # use local environment\n",
    "        # \"work_dir\": \"/temp\",\n",
    "        \"last_n_messages\": 3,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize agent selction function\n",
    "Inspired by StateFlow (Wu et al. 2024), a LLM-based task-solving paradigm that conceptualizes complex task-solving processes as state machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized agent selection function\n",
    "def speaker_selection(last_speaker, groupchat):\n",
    "    messages = groupchat.messages\n",
    "    \n",
    "    if len(messages) <= 1:  \n",
    "        # Initial state: user starts conversation with Manager\n",
    "        return manager\n",
    "    \n",
    "    if last_speaker == manager:\n",
    "        # Manager communicates directly with the user\n",
    "        return user_proxy\n",
    "    \n",
    "    if last_speaker == user_proxy:\n",
    "        # After Manager's communication, if user approves the report\n",
    "        if \"Approve\" in messages[-1][\"content\"]:\n",
    "            return scientist  # Pass the report to Scientist upon user approval\n",
    "        elif \"End\" in messages[-1][\"content\"]:\n",
    "            return None  # End the chat if user requests\n",
    "        return manager  # Continue conversation with Manager if user hasn't approved\n",
    "    \n",
    "    # State 2: Transition from Scientist to Engineer for a well-defined research question\n",
    "    if last_speaker == scientist:\n",
    "        # Scientist hands over the task to Engineer\n",
    "        return engineer\n",
    "    \n",
    "    if last_speaker == engineer:\n",
    "        # Engineer generates code and passes it to Executor for execution\n",
    "        return executor\n",
    "    \n",
    "    if last_speaker == executor:\n",
    "        # Executor executes the code and checks for errors\n",
    "        if any(keyword in messages[-1][\"content\"] for keyword in [\"error\", \"failed\", \"exception\"]):\n",
    "            return engineer  # Return to Engineer for modifications if there are errors\n",
    "        elif messages[-1][\"content\"] == \"\":\n",
    "            return engineer  # Return to Engineer if output is empty\n",
    "        else:\n",
    "            return manager  # Return results to Manager if execution is successful\n",
    "\n",
    "    return None  # End the conversation if none of the above conditions are met\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group chat setting\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, manager, scientist, engineer, executor], \n",
    "    messages=[], \n",
    "    max_round=30,\n",
    "     speaker_selection_method=speaker_selection\n",
    ")\n",
    "\n",
    "\n",
    "# chat administrator (group chat manager)\n",
    "chat_admin = autogen.GroupChatManager(\n",
    "    name=\"Chat_Admin\",\n",
    "    llm_config=gpt4_config,\n",
    "    groupchat = groupchat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start chatting\n",
    "Input your request and run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating chat\n",
    "users_request = 'I am a researcher in the urban greening department. I want to know the health condition of urban vegetation in my district.' #TODO: enter user's request here\n",
    "\n",
    "user_proxy.initiate_chat(\n",
    "    chat_admin,\n",
    "    message= users_request,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
