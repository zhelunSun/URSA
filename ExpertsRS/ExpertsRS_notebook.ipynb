{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for ExpertsRS\n",
    "This is a demo notebook for running our ExpertsRS prototype system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the API\n",
    "Set up your LLM api and URL, user can include api for different models in the [config list](llm_config_list.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to sys.path\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from prompts import scientist_prompt, engineer_prompt, manager_prompt, user_proxy_prompt, executor_prompt\n",
    "\n",
    "config_list = autogen.config_list_from_json(\"llm_config_list.json\") # set the api key in the config list\n",
    "print(\"LLM models: \", [config_list[i][\"model\"] for i in range(len(config_list))]) # print the list of LLM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build filters to select different LLM models based on tags\n",
    "llm_filters = {\n",
    "    \"gpt-4\": {\"tags\": [\"gpt-4\"]},\n",
    "    \"deepseek-v3\": {\"tags\": [\"deepseek-v3\"]},\n",
    "    \"deepseek-r1\": {\"tags\": [\"deepseek-r1\"]},\n",
    "}\n",
    "\n",
    "# Filter config_list for each model\n",
    "gpt4_config_list = autogen.filter_config(config_list, llm_filters[\"gpt-4\"])\n",
    "deepseekv3_config_list = autogen.filter_config(config_list, llm_filters[\"deepseek-v3\"])\n",
    "deepseekr1_config_list = autogen.filter_config(config_list, llm_filters[\"deepseek-r1\"])\n",
    "\n",
    "# Ensure only one config matched for deepseek-v3\n",
    "assert len(deepseekv3_config_list) == 1, \"deepseek-v3 config_list must contain exactly 1 config.\"\n",
    "\n",
    "# Build llm_config dict for each model\n",
    "# gpt-4\n",
    "gpt4_config = {\n",
    "    \"cache_seed\": 1,\n",
    "    \"temperature\": 0,\n",
    "    \"timeout\": 180,\n",
    "    \"config_list\": gpt4_config_list,\n",
    "}\n",
    "\n",
    "# deepseek-v3\n",
    "deepseekv3_config = {\n",
    "    \"cache_seed\": 100,\n",
    "    \"temperature\": 0,\n",
    "    \"timeout\": 180,\n",
    "    \"config_list\": deepseekv3_config_list,\n",
    "}\n",
    "\n",
    "# deepseek-r1\n",
    "deepseekr1_config = {\n",
    "    \"cache_seed\": 1,\n",
    "    \"temperature\": 0,\n",
    "    \"timeout\": 180,\n",
    "    \"config_list\": deepseekr1_config_list,\n",
    "}\n",
    "\n",
    "# Double-check the model\n",
    "print(\"[INFO] LLM models in config_list:\")\n",
    "print(\" - GPT-4:\", [cfg[\"model\"] for cfg in gpt4_config_list])\n",
    "print(\" - DeepSeek-V3:\", [cfg[\"model\"] for cfg in deepseekv3_config_list])\n",
    "print(\" - DeepSeek-R1:\", [cfg[\"model\"] for cfg in deepseekr1_config_list])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The User proxy\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    system_message= user_proxy_prompt,\n",
    "    code_execution_config=False,\n",
    "    human_input_mode=\"ALWAYS\",\n",
    ")\n",
    "\n",
    "# The Manager\n",
    "manager = autogen.AssistantAgent(\n",
    "    name=\"Manager\",\n",
    "    system_message= manager_prompt,\n",
    "    llm_config=deepseekv3_config\n",
    ")\n",
    "\n",
    "# The Scientist\n",
    "scientist = autogen.AssistantAgent(\n",
    "    name=\"Scientist\",\n",
    "    system_message= scientist_prompt,\n",
    "    llm_config=deepseekv3_config\n",
    ")\n",
    "\n",
    "# The Engineer\n",
    "engineer = autogen.AssistantAgent(\n",
    "    name=\"Engineer\",\n",
    "    system_message=engineer_prompt,\n",
    "    llm_config=deepseekv3_config,\n",
    "    code_execution_config=False,  # Turn off code execution for this agent.\n",
    "\n",
    ")\n",
    "\n",
    "# The Executor\n",
    "executor = autogen.UserProxyAgent(\n",
    "    name=\"Executor\",\n",
    "    system_message=executor_prompt,\n",
    "    human_input_mode=\"NEVER\",  # auto response mode\n",
    "     code_execution_config={\n",
    "        \"executor\": \"commandline-local\",  # use local environment\n",
    "        \"last_n_messages\": 3,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize agent selction function\n",
    "Inspired by [StateFlow](https://arxiv.org/abs/2403.11322), a LLM-based task-solving paradigm that conceptualizes complex task-solving processes as state machines.\n",
    "\n",
    "ExpertsRS operates through **four states** : \n",
    "- 1. Clarify request: The Manager engages with the User to transform vague inputs into a structured request\n",
    "- 2. Define problem: The Scientist formulates a detailed statement outlining objectives, data needs, and methods\n",
    "- 3. Solve problem: The engineer agent breaks the statement into subtasks, generates executable code, and submits it to the Executor. If errors arise, messages are routed to the Engineer for iterative code refinement until successful execution.\n",
    "- 4. Generate report: The Manager compiles a user-friendly report incorporating the analytical process, maps, indicators, and other explainations.\n",
    "\n",
    "Check section 3.1 and 4.1 in the paper for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized agent selection function\n",
    "def speaker_selection(last_speaker, groupchat):\n",
    "    messages = groupchat.messages\n",
    "    \n",
    "    if len(messages) <= 1:  \n",
    "        # Initial state: user starts conversation with Manager\n",
    "        return manager\n",
    "    \n",
    "    if last_speaker == manager:\n",
    "        # Manager communicates directly with the user\n",
    "        return user_proxy\n",
    "    \n",
    "    if last_speaker == user_proxy:\n",
    "        # After Manager's communication, if user approves the report\n",
    "        if \"Approve\" in messages[-1][\"content\"]:\n",
    "            return scientist  # Pass the report to Scientist upon user approval\n",
    "        elif \"End\" in messages[-1][\"content\"]:\n",
    "            return None  # End the chat if user requests\n",
    "        return manager  # Continue conversation with Manager if user hasn't approved\n",
    "    \n",
    "    # State 2: Transition from Scientist to Engineer for a well-defined research question\n",
    "    if last_speaker == scientist:\n",
    "        # Scientist hands over the task to Engineer\n",
    "        return engineer\n",
    "    \n",
    "    if last_speaker == engineer:\n",
    "        # Engineer generates code and passes it to Executor for execution\n",
    "        return executor\n",
    "    \n",
    "    if last_speaker == executor:\n",
    "        # Executor executes the code and checks for errors\n",
    "        if any(keyword in messages[-1][\"content\"] for keyword in [\"error\", \"failed\", \"exception\"]):\n",
    "            return engineer  # Return to Engineer for modifications if there are errors\n",
    "        elif messages[-1][\"content\"] == \"\":\n",
    "            return engineer  # Return to Engineer if output is empty\n",
    "        else:\n",
    "            return manager  # Return results to Manager if execution is successful\n",
    "\n",
    "    return None  # End the conversation if none of the above conditions are met\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group chat setting\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, manager, scientist, engineer, executor], \n",
    "    messages=[], \n",
    "    max_round=30,\n",
    "     speaker_selection_method=speaker_selection\n",
    ")\n",
    "\n",
    "\n",
    "# chat administrator (group chat manager)\n",
    "chat_admin = autogen.GroupChatManager(\n",
    "    name=\"Chat_Admin\",\n",
    "    llm_config=deepseekv3_config,\n",
    "    groupchat = groupchat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start chatting\n",
    "Input your request and run this cell. \n",
    "\n",
    "Chat with the Manager to express your need sufficiently, if satisfied, type 'Approve' to enter the following states.\n",
    "\n",
    "Type 'End' or 'exit' to end the chat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating chat\n",
    "users_request = 'I am an official in the urban greening department. I want a map of greenspace to support my policy-making.' #TODO: enter user's request here\n",
    "\n",
    "user_proxy.initiate_chat(\n",
    "    chat_admin,\n",
    "    message= users_request,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
